---
title: "crime_weather_covid_Forecasting"
author: "Madlen Wilmes"
date: "6/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load packages

```{r eval=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(bigrquery)
library(ggplot2)
library(tidyr)
library(lubridate) # handle date information
library(readr) # read txt file with credentials
library(fable)
#devtools::install_github("tidyverts/fabletools", force = TRUE)
library(fabletools)
library(tsibble)
library(feasts)
library(ggpmisc) # plot R^2 in scatter plot
```

### Retrieve Data from BigQuery
```{r get crime data}
source("crime_weather_covid_ReadData.R")
# daily_counts <- get_crime_per_day()
daily_counts_per_type <- get_crime_counts_per_type_and_day()
daily_weather <- get_weather_data()
```

```{r join and save}
# left join throws (strange) error if length of tsibble not specified
daily_crime_weather_per_type <- left_join(daily_counts_per_type[1:nrow(daily_counts_per_type),], daily_weather, by = "date")
write.csv(daily_crime_weather_per_type, "crime_weather_day_per_type.csv", row.names = FALSE)
```

## Read csv, if you've aleady retrieved data today
```{r read csv with crime and weather per type and day}
daily_counts_per_type <- read.csv("crime_weather_day_per_type.csv")
daily_counts_per_type$date <- as.Date(daily_counts_per_type$date)
daily_counts_per_type <- as_tsibble(daily_counts_per_type, index=date, key=c(iucr, primary_type, description)) %>% arrange(date)
```

# Simple Forecasting methods

## Subset data for easier exploring

```{r create simple data set}
## monthly counts of simple assault cases
monthly_simple_assault <- daily_counts_per_type %>% 
      group_by(iucr) %>%
      index_by(year_month = yearmonth(date)) %>% # monthly aggregates
      summarise(avg_counts = mean(counts, na.rm = TRUE)) %>%
      filter(iucr == '0560')
monthly_simple_assault <- monthly_simple_assault[,-1]
```


## Average method

The forecasts of all future values are equal to the average (or “mean”) of the historical data.

```{r}
monthly_simple_assault %>%  
  model(MEAN(avg_counts)) %>% 
  forecast(h = "36 months") %>% 
  autoplot(monthly_simple_assault, level = NULL) + # don't plot the confidence bands
     ggtitle("Monthly average counts of simple assault offenses")
```


## Naïve method

All forecasts are the same value as the last observation.

```{r}
monthly_simple_assault %>%  
  model(NAIVE(avg_counts)) %>% 
  forecast(h = "36 months") %>% 
  autoplot(monthly_simple_assault, level = NULL) + # don't plot the confidence bands
     ggtitle("Monthly average counts of simple assault offenses")
```


## Seasonal naïve method

Set each forecast to be equal to the observed value from the same season of the last period (e.g., the same month of the previous year).

```{r}
monthly_simple_assault %>%  
  model(SNAIVE(avg_counts ~ lag("year"))) %>% 
  forecast(h = "3 years") %>% 
  autoplot(monthly_simple_assault, level = NULL) # don't plot the confidence bands
```

## Drift method

A variation on the naïve method.  Allow the forecasts to increase or decrease into the future. The amount of change over time (called the drift) is set to the average change in the historical data.

```{r}
monthly_simple_assault %>%  
  model(RW(avg_counts ~ drift())) %>% 
  forecast(h = "3 years") %>% 
  autoplot(monthly_simple_assault, level = NULL) # don't plot the confidence bands
```

## Show all methods in one graph

```{r fit all simple models and plot}
# Set training data from 2001 to 2017
train <- monthly_simple_assault %>% filter_index("2001" ~ "2017")
# Fit the models
assault_fit <- train %>%
  model(
    Mean = MEAN(avg_counts),
    `Naïve` = NAIVE(avg_counts),
    `Seasonal naïve` = SNAIVE(avg_counts),
    `Drift` = NAIVE(avg_counts ~ drift())
  )
# Generate forecasts for 4 years (i.e., 48 mo)
assault_fc <- assault_fit %>% forecast(h=48)
# Plot forecasts against actual values
assault_fc %>%
  autoplot(monthly_simple_assault, level = NULL) +
    autolayer(filter_index(monthly_simple_assault, "2001" ~ .), color = "black") +
    ggtitle("Forecasts for simple assault cases") +
    xlab("Year") + ylab("No. of offenses") +
    guides(colour=guide_legend(title="Forecast"))
```

```{r retrive fitted and residual values}
augment(assault_fit)
```

## Residual Diagnostics

A good forecasting method will yield residuals with the following properties:

- The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
- The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

Checking these properties is important in order to see whether a method is using all of the available information, but it is not a good way to select a forecasting method (i.e., determine it's goodness of fit.)

Adjusting for bias is easy: if the residuals have mean, m, then simply add m to all forecasts and the bias problem is solved. Fixing a correlation problem is a different matter (not addressed right now)

Furthermore:
- The residuals have constant variance.
- The residuals are normally distributed.

However, a forecasting method that does not satisfy these properties cannot necessarily be improved. Sometimes applying a Box-Cox transformation may assist with these properties, but otherwise there is usually little that you can do to ensure that your residuals have constant variance and a normal distribution. Instead, apply an alternative approach for prediction intervals (addressed later)

## Plot residuals

```{r}
aug <- augment(assault_fit)
aug %>% autoplot(.resid) + xlab("Day") + ylab("Residuals") +
  facet_grid(.model ~ .) +
  ggtitle("Residuals of the four simple forecasting methods") +
  theme_bw()
```

Time plot of the residuals across the historical data. The variance of the residuals of the naive and mean models is more or less constant. The variance of the mean and seasonal naive model is not constant (i.e., the model does not capture the variance in the raw data well.)


## Distribution of residuals

```{r}
# calculate the mean of the residual (is it different from zero?)
mu <- aug %>% as_tibble() %>% group_by(.model) %>% summarise(grp.mean = mean(.resid, na.rm = TRUE))

aug %>%
  # filter(.model != 'Drift') %>%
  ggplot(aes(x = .resid, fill = .model)) +
  geom_density(alpha=0.25) +
  facet_grid(.model ~ .) +
  ggtitle("Density plot of residuals") +
  # add line for mean
  geom_vline(data=mu, aes(xintercept=grp.mean, color="red"), linetype="dashed")
```
Note that the resituals of the drift and naive models are identical.
The drift and naive model center around zero and have too long tails to be considered a normal distribution. The residuals of the seasonal naive model is shifted. Forecasts from a model with approximate normal distribution might be good, but prediction intervals (that assume a normal distribution) may be inaccurate.

## Check for autocorrelation in residuals

```{r plot autocorrelation of four simple models}
aug  %>% ACF(.resid) %>% autoplot() + ggtitle("ACF of residuals")
```
The residuals of all four models show (severe) autocorrelation (i.e., the models do not account for all variation and could be improved.)

## Produce diagnostics graphs in one line

```{r}
train %>% model(SNAIVE(avg_counts)) %>% gg_tsresiduals()
```

## Portmanteau test

Check autocorrelation formally (i.e., not visually as above) using a portmanteau test that also account for multiple testing. It tests if multiple correlation values (at different lags) are statistically larger than expected under a white noice scenario.

There are multiple variations of portmanteay tests: for example Box-Pierce test and Ljung-Box test.

```{r test if residuals are autocorrelated using Box-Pierce test}
# lag=h and fitdf=K
aug %>%  features(.resid, box_pierce, lag=10, dof=0)
```

```{r test if residuals are autocorrelated using Ljung-Box test}
# lag=h and fitdf=K
aug %>% features(.resid, ljung_box, lag=10, dof=0)
```
As deduced from the ACF plots above, both tests confirm that the residuals of all four models have significant auto-correlation (bp_pvalue = 0).

If your model estimates parameters (such as the drift model), remember to set the dof (degree of freedom) accordingly (i.e., drift model estimates one parameter so dof in the portmanteau test should be set to 1).

## Estimating prediction intervals 

When forecasting one step ahead, the standard deviation of the forecast distribution is almost the same as the standard deviation of the residuals. In fact, the two standard deviations are identical if there are no parameters to be estimated, as is the case with the naïve method. For forecasting methods involving parameters to be estimated (e.g., drift method), the standard deviation of the forecast distribution is slightly larger than the residual standard deviation, although this difference is often ignored.

We can mathematically derive the forecast standard deviation under the assumption of uncorrelated residuals. Prediction intervals can easily be computed using the fable package. By default, the 80% and 95% certainty levels are shown (but can be adjusted using the `level` argument).
```{r}
train %>%
  model(SNAIVE(avg_counts)) %>%
  forecast(h = 10) %>%
  autoplot(train)
```

When the distribution of the resituals do not follow a normal distribution (but are uncorrelated), you can use bootstrapping. Bootstrapping assumes that the future error is similar to the error in the past. So you randomly select from the past residuals to determine the forcasting error.

```{r bootstrapping}
fit <- train %>% model(SNAIVE(avg_counts))
fc <- fit %>% forecast(h = 10, bootstrap = TRUE)
fc %>% autoplot(train) +
  ggtitle("Prediction of simple assault cases (seasonal naive method)")
```
Because there is no normality assumption, the prediction intervals are not symmetric.

## Estimating forecasting error

- forecast “error” is the difference between an observed value and its forecast
- forecast errors are different from residuals: (1) residuals are calculated on the training set while forecast errors are calculated on the test set (2) residuals are based on one-step forecasts while forecast errors can involve multi-step forecasts
- split data into training and test set
- test set approximately 20% of complete data set (at least as large as forecast)
- use `dplyr` functions `filter` and `slice` to split the data

```{r slice example}
# use last 19 months for test set
test_set <- monthly_simple_assault %>%
  slice(n()-19:0)
# slice also works on groups
```

There are several ways to estimate forecasting error:

*scale-dependent measures* (i.e., only compare models for data with identical units)
Mean absolute error: MAE  --> forecast method that minimises the MAE will lead to forecasts of the median
Root mean squared error: RMSE --> minimising the RMSE will lead to forecasts of the mean

*unit-free errors* (i.e., useful to compare forecast performances between data sets)
Mean absolute percentage error: MAPE
Problems: 
- MAPE is infinite or undefined if y_t = 0 for any t in the series, and having extreme values if any y_t is close to zero.
- assumes the unit of measurement has a meaningful zero (not the case for temperature in C or F!)
- puts a heavier penalty on negative errors than on positive errors --> “symmetric” MAPE (sMAPE) corrects that but is flawed (don't use)

*scaled errors*
- mean absolute scaled error (MASE)
- alternative to using percentage errors when comparing forecast accuracy across series with different units
- scale errors based on the training MAE from a simple forecast method (e.g., naive, seasonal naive)
- scaled error is less than one if it arises from a better forecast than the average naïve forecast computed on the training data
- greater than one if the forecast is worse than the average naïve forecast computed on the training data

```{r}
train_set <- monthly_simple_assault %>% filter_index("2001" ~ "2018")

# Fit the models
assault_fit <- train_set %>%
  model(
    Mean = MEAN(avg_counts),
    `Naïve` = NAIVE(avg_counts),
    `Seasonal naïve` = SNAIVE(avg_counts),
    `Drift` = RW(avg_counts ~ drift())
  )
# Generate forecasts for 28 mo (the lenght of the test set)
assault_fc <- assault_fit %>% forecast(h=28)

# Function throws error (also with minimal example) --> contacted Rob Hyndman
accuracy(assault_fc, monthly_simple_assault)
```
The Seasonal naive model has the smallest (absolute) value, and, hence, performs best. Note that not all measures may provide the same answer.

## Cross-validation
- testing a series of test sets (adding in more and more values)
- each (out-of-sample) test set has a single observation (the obervation following the training set)
- forecast accuracy is computed by averaging over the test sets


### Compare the accuracy obtained via time series cross-validation with the residual accuracy

```{r }
# Time series cross-validation accuracy
simple_assault_train <- monthly_simple_assault %>%
  slice(1:(n()-1)) %>%
  # start with at least 5 points, proceed by one step
  stretch_tsibble(.init = 5, .step = 1)
  
fc <- simple_assault_train %>%
  model(SNAIVE(avg_counts)) %>%
  forecast(h=1)

fc %>% accuracy(monthly_simple_assault)
```

```{r print residual accuracy for comparison}
# Residual accuracy
monthly_simple_assault %>% model(SNAIVE(avg_counts)) %>% accuracy()
```

Take-home: forecasting error (from residuals) judges how well a particular model fits the data (how close does the predicted value lay to the measured value). Cross-validation makes a statement how well our model forecasts values that are not part of the training set. If the forecasting error and cross-validation are of similar size, we are not overfitting (i.e., we didn't make our model fit the data super tight, but lowered ability to forecast out-of-sample/future data).