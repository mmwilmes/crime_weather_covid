---
title: "crime_weather_covid_Forecasting"
author: "Madlen Wilmes"
date: "6/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Retrieve Data from BigQuery
```{r get crime data}
source("crime_weather_covid_ReadData.R")
# daily_counts <- get_crime_per_day()
daily_counts_per_type <- get_crime_counts_per_type_and_day()
daily_weather <- get_weather_data()
```

```{r join and save}
# left join throws (strange) error if length of tsibble not specified
  daily_crime_weather_per_type <- left_join(daily_counts_per_type[1:nrow(daily_counts_per_type),], daily_weather, by = "date")
  write.csv(daily_crime_weather_per_type, "crime_weather_day_per_type.csv", row.names = FALSE)
```

## Read csv, if you've aleady retrieved data today
```{r read csv with crime and weather per type and day}
daily_counts_per_type <- read.csv("crime_weather_day_per_type.csv")
daily_counts_per_type$date <- as.Date(daily_counts_per_type$date)
daily_counts_per_type <- as_tsibble(daily_counts_per_type, index=date, key=c(iucr, primary_type, description)) %>% arrange(date)
```

# Simple Forecasting methods

## Subset data for easier exploring

```{r create simple data set}
## monthly counts of simple assault cases
monthly_simple_assault <- daily_counts_per_type %>% 
      group_by(iucr) %>%
      filter(iucr == '0560') %>%
      index_by(year_month = yearmonth(date)) %>% # monthly aggregates
      summarise(avg_counts = mean(counts, na.rm = TRUE))
monthly_simple_assault <- monthly_simple_assault[,-1]
```


## Average method

The forecasts of all future values are equal to the average (or “mean”) of the historical data.

```{r}
monthly_simple_assault %>%  
  model(MEAN(avg_counts)) %>% 
  forecast(h = "36 months") %>% 
  autoplot(monthly_simple_assault, level = NULL) + # don't plot the confidence bands
     ggtitle("Monthly average counts of simple assault offenses")
```


## Naïve method

All forecasts are the same value as the last observation.

```{r}
monthly_simple_assault %>%  
  model(NAIVE(avg_counts)) %>% 
  forecast(h = "36 months") %>% 
  autoplot(monthly_simple_assault, level = NULL) + # don't plot the confidence bands
     ggtitle("Monthly average counts of simple assault offenses")
```


## Seasonal naïve method

Set each forecast to be equal to the observed value from the same season of the last period (e.g., the same month of the previous year).

```{r}
monthly_simple_assault %>%  
  model(SNAIVE(avg_counts ~ lag("year"))) %>% 
  forecast(h = "3 years") %>% 
  autoplot(monthly_simple_assault, level = NULL) # don't plot the confidence bands
```

## Drift method

A variation on the naïve method.  Allow the forecasts to increase or decrease into the future. The amount of change over time (called the drift) is set to the average change in the historical data.

```{r}
monthly_simple_assault %>%  
  model(RW(avg_counts ~ drift())) %>% 
  forecast(h = "3 years") %>% 
  autoplot(monthly_simple_assault, level = NULL) # don't plot the confidence bands
```

## Show all methods in one graph

```{r}
# Set training data from 2001 to 2017
train <- monthly_simple_assault %>% filter_index("2001" ~ "2017")
# Fit the models
assault_fit <- train %>%
  model(
    Mean = MEAN(avg_counts),
    `Naïve` = NAIVE(avg_counts),
    `Seasonal naïve` = SNAIVE(avg_counts),
    `Drift` = NAIVE(avg_counts ~ drift())
  )
# Generate forecasts for 4 years (i.e., 48 mo)
assault_fc <- assault_fit %>% forecast(h=48)
# Plot forecasts against actual values
assault_fc %>%
  autoplot(monthly_simple_assault, level = NULL) +
    autolayer(filter_index(monthly_simple_assault, "2001" ~ .), color = "black") +
    ggtitle("Forecasts for simple assault cases") +
    xlab("Year") + ylab("No. of offenses") +
    guides(colour=guide_legend(title="Forecast"))
```

```{r retrive fitted and residual values}
augment(assault_fit)
```

## Residual Diagnostics

A good forecasting method will yield residuals with the following properties:

- The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
- The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

Checking these properties is important in order to see whether a method is using all of the available information, but it is not a good way to select a forecasting method (i.e., determine it's goodness of fit.)

Adjusting for bias is easy: if the residuals have mean, m, then simply add m to all forecasts and the bias problem is solved. Fixing a correlation problem is a different matter (not addressed right now)

Furthermore:
- The residuals have constant variance.
- The residuals are normally distributed.

However, a forecasting method that does not satisfy these properties cannot necessarily be improved. Sometimes applying a Box-Cox transformation may assist with these properties, but otherwise there is usually little that you can do to ensure that your residuals have constant variance and a normal distribution. Instead, apply an alternative approach for prediction intervals (addressed later)

## Plot residuals

```{r}
aug <- augment(assault_fit)
aug %>% autoplot(.resid) + xlab("Day") + ylab("Residuals") +
  facet_grid(.model ~ .) +
  ggtitle("Residuals of the four simple forecasting methods") +
  theme_bw()
```

Time plot of the residuals across the historical data. The variance of the residuals of the naive and mean models is more or less constant. The variance of the mean and seasonal naive model is not constant (i.e., the model does not capture the variance in the raw data well.)


## Distribution of residuals

```{r}
# calculate the mean of the residual (is it different from zero?)
mu <- aug %>% as_tibble() %>% group_by(.model) %>% summarise(grp.mean = mean(.resid, na.rm = TRUE))

aug %>%
  # filter(.model != 'Drift') %>%
  ggplot(aes(x = .resid, fill = .model)) +
  geom_density(alpha=0.25) +
  facet_grid(.model ~ .) +
  ggtitle("Density plot of residuals") +
  # add line for mean
  geom_vline(data=mu, aes(xintercept=grp.mean, color="red"), linetype="dashed")
```
Note that the resituals of the drift and naive models are identical.
The drift and naive model center around zero and have too long tails to be considered a normal distribution. The residuals of the seasonal naive model is shifted. Forecasts from a model with approximate normal distribution might be good, but prediction intervals (that assume a normal distribution) may be inaccurate.

## Check for autocorrelation in residuals

```{r plot autocorrelation of four simple models}
aug  %>% ACF(.resid) %>% autoplot() + ggtitle("ACF of residuals")
```
The residuals of all four models show (severe) autocorrelation (i.e., the models do not account for all variation and could be improved.)

## Produce diagnostics graphs in one line

```{r}
train %>% model(SNAIVE(avg_counts)) %>% gg_tsresiduals()
```

## Portmanteau test --> 

Check autocorrelation formally (i.e., not visually as above) using a portmanteau test that also account for multiple testing. It tests if multiple correlation values (at different lags) are statistically larger than expected under a white noice scenario.

There are multiple variations of portmanteay tests: for example Box-Pierce test and Ljung-Box test.

```{r test if residuals are autocorrelated using Box-Pierce test}
# lag=h and fitdf=K
aug %>%  features(.resid, box_pierce, lag=10, dof=0)
```

```{r test if residuals are autocorrelated using Ljung-Box test}
# lag=h and fitdf=K
aug %>% features(.resid, ljung_box, lag=10, dof=0)
```
As deduced from the ACF plots above, both tests confirm that the residuals of all four models have significant auto-correlation (bp_pvalue = 0).

If your model estimates parameters (such as the drift model), remember to set the dof (degree of freedom) accordingly (i.e., drift model estimates one parameter so dof in the portmanteau test should be set to 1).

## Estimating error of a forcast

When forecasting one step ahead, the standard deviation of the forecast distribution is almost the same as the standard deviation of the residuals. In fact, the two standard deviations are identical if there are no parameters to be estimated, as is the case with the naïve method. For forecasting methods involving parameters to be estimated (e.g., drift method), the standard deviation of the forecast distribution is slightly larger than the residual standard deviation, although this difference is often ignored.

We can mathematically derive the forecast standard deviation under the assumption of uncorrelated residuals. Prediction intervals can easily be computed using the fable package. By default, the 80% and 95% certainty levels are shown (but can be adjusted using the `level` argument).
```{r}
train %>%
  model(SNAIVE(avg_counts)) %>%
  forecast(h = 10) %>%
  autoplot(train)
```

When the distribution of the resituals do not follow a normal distribution (but are uncorrelated), you can use bootstrapping. Bootstrapping assumes that the future error is similar to the error in the past. So you randomly select from the past residuals to determine the forcasting error.

```{r bootstrapping}
fit <- train %>% model(SNAIVE(avg_counts))
fc <- fit %>% forecast(h = 10, bootstrap = TRUE)
fc %>% autoplot(train) +
  ggtitle("Prediction of simple assault cases (seasonal naive method)")
```
Because there is no normality assumption, the prediction intervals are not symmetric.