---
title: "crime_weather_covid_ReadData"
author: "Madlen Wilmes"
date: "6/1/2020"
output: html_document
---

```{r Markdown cheatsheet, echo=FALSE}
# echo=FALSE prevents code, but not the results from appearing in the rendered file
# eval=TRUE evaluates the code and includes the results
# results='hide' hides the results but displays the code
```


### Install packages (if required)

```{r install packages, eval=FALSE, message=FALSE, warning=FALSE} 
# set eval=TRUE if running script for the first time
if(!require("ggseas")) install.packages("ggseas")
if(!require("forecast")) install.packages("forecast")
if(!require("data.table")) install.packages("data.table")
if(!require("knitr")) install.packages("knitr")
if(!require("devtools")) install.packages("devtools") 
if(!require("bigrquery")) install.packages("bigrquery")
if(!require("tsibble")) install.packages("tsibble")
if(!require("fable")) install.packages("fable")  # forecasting package of the tidyverse family
# devtools::install_github("rstats-db/bigrquery", force = TRUE)
```

### Load packages

```{r eval=TRUE, message=FALSE, warning=FALSE}
library(bigrquery)
library(ggplot2)
library(tidyr)
library(dplyr)
library(lubridate) # handle date information
library(readr) # read txt file with credentials
library(fable)
library(tsibble)
library(feasts)
library(ggpmisc) # plot R^2 in scatter plot
```

### Retrieve Google Cloud credentials
Retrieve Google Cloud project ID, which I store in an external file (not uploaded to github).
```{r set up credentials, eval = FALSE}
# Set Google Cloud project ID here
project_name <- read_file("./GC_credentials.txt")
```


### Retrieve data with date information

```{r setup request, eval = FALSE, echo = FALSE, message=FALSE, warning=FALSE}
sql <- "
SELECT 
  iucr,
  primary_type,
  description, 
  date
FROM `bigquery-public-data.chicago_crime.crime` 
"
```

TODO: Cache locally and only fetch additional (recent) data.

```{r execute SQL call, eval = FALSE, echo = FALSE}
# Execute the query and store the result
counts_day <- query_exec(sql, project = project_name, use_legacy_sql = FALSE, max_pages = Inf)
str(counts_day)
```

Each row in the data stands for one offense of a particular type, at a particular time. We add a column of "counts", that for now is always one. This fascilitates summing up rows (e.g., by day) later on.
```{r add colum of ones --> one row is one offense count}
counts_day$count <- rep(1,nrow(counts_day))
# format as date (and drop the time part)
counts_day$date <- as.Date(ymd_hms(counts_day$date))

# sort by ascending date
counts_day <- counts_day %>% arrange(date)
```

```{r aggregate counts per day}
# group by day (across offense types)
counts_day <- tibble(counts_day)

daily_counts <- counts_day %>%
  group_by(date) %>%
  summarize(counts = sum(count))
head(daily_counts)
```

The sum of all days should be equal to the initial number of rows of data (i.e., we didn't loose any information in the above data transformation)
```{r sanitiy check}
sum(daily_counts$counts) == nrow(counts_day)
```

### tsibble: time-sensitive tibble
1. Index is a variable with inherent ordering from past to present.
2. Key is a set of variables that define observational units over time.
3. Each observation should be uniquely identified by index and key.
4. Each observational unit should be measured at a common interval, if regularly spaced.
```{r time-aware tibble}
# turn into time-sensitive tibble
# works like a normal tibble but tracks time (set by index)
daily_counts <- as_tsibble(daily_counts, index=date) %>% arrange(date)
```


## Retrieve the weather data

Before pulling data into R, I used the BigQuery online UI to explore available stations. Most stations have a restricted runtime. That means no single station in Chicago ran the entire time period for which we have crime information (01-01-2001 to today).

First, I explored the 'stations' table and realized that multiple stations report weather data for the city Chicago. We can create a geographical quadrant (using lon/lat to select all stations that report data for our crime area.)

SELECT usaf
FROM bigquery-public-data.noaa_gsod.stations
WHERE state='IL' AND (`lat` BETWEEN 41.6 AND 42) AND (`lon` BETWEEN -88 AND -87)
  AND `end` > '20010000'
ORDER BY `end` ASC;

Eventually, I identified the station named "Chicago" (usaf = '997338') as the station with the longest period of continous weather data (20080101 to last full week from today). The station "CHICAGO/MEIGS" (usaf = '725346') reports data from 19730101 to 20080618 and is, hence, useful to fill in the data from 2001 to 2007.


## Retrieve weather data with date information
```{r setup request}
sql <- "
#standardsql
SELECT
  -- Create a timestamp from the date components.
  timestamp(concat(year,'-',mo,'-',da)) as date,
  -- Replace numerical null values with actual nulls
  ROUND(AVG(IF (temp=9999.9, null, temp)),2) AS temperature,
  ROUND(AVG(IF (visib=999.9, null, visib)),2) AS visibility,
  ROUND(AVG(IF (wdsp='999.9', null, CAST(wdsp AS Float64))),2) AS wind_speed,
  ROUND(AVG(IF (gust=999.9, null, gust)),2) AS wind_gust,
  ROUND(AVG(IF (prcp=99.99, null, prcp)),3) AS precipitation,
  AVG(IF (sndp=999.9, null, sndp)) AS snow_depth
FROM
  `bigquery-public-data.noaa_gsod.gsod*`
WHERE
   `stn` IN 
  (SELECT `usaf`
    FROM bigquery-public-data.noaa_gsod.stations
    WHERE state='IL' AND (`lat` BETWEEN 41.6 AND 42) AND (`lon` BETWEEN -88 AND -87)
      AND `end` > '20010000'
    ORDER BY `end` ASC) AND CAST(YEAR AS INT64) > 2000
 GROUP BY date;
"
```


TODO: Cache locally and only fetch additional (recent) data.

```{r execute SQL call}
# Execute the query and store the result
weather <- query_exec(sql, project = project_name, use_legacy_sql = FALSE, max_pages = Inf)
weather <- arrange(weather, weather$date)
str(weather)
```

How many days of data do we expect?
```{r}
max(weather$date) - min(weather$date)
# as.Date("2001-01-01", format="%Y-%m-%d") - as.Date("2020-05-13", format="%Y-%m-%d")
```

Only using data from all stations across Chicago, ensures that we have no gaps in the data. The SQL query above averages across stations per day.

However, we have one more row than expeted. TODO


Turn data frame into a tsibble.

```{r transform into tsibble}
weather$date <- as.Date(weather$date)
daily_weather <- as_tsibble(weather, index = date, regular = TRUE, interval = day)
```

#  Join the crime and weather data
```{r join crime and weather data}
daily_crime_weather<- left_join(daily_counts, daily_weather, by = "date")
```


```{r write data to csv}
write.csv(daily_crime_weather, "crime_weather_day.csv", row.names = FALSE)
```
