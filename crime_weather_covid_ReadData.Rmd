---
title: "crime_weather_covid_ReadData"
author: "Madlen Wilmes"
date: "6/1/2020"
output: html_document
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Markdown cheatsheet, echo=FALSE, purl=FALSE}
# echo=FALSE prevents code, but not the results from appearing in the rendered file
# eval=TRUE evaluates the code and includes the results
# results='hide' hides the results but displays the code
```


### Install packages (if required)

```{r install packages, eval=FALSE, message=FALSE, warning=FALSE} 
# set eval=TRUE if running script for the first time
if(!require("dplyr")) install.packages("dplyr")
if(!require("ggseas")) install.packages("ggseas")
if(!require("forecast")) install.packages("forecast")
if(!require("data.table")) install.packages("data.table")
if(!require("knitr")) install.packages("knitr")
if(!require("devtools")) install.packages("devtools") 
if(!require("bigrquery")) install.packages("bigrquery")
if(!require("tsibble")) install.packages("tsibble")
if(!require("fable")) install.packages("fable")  # forecasting package of the tidyverse family
```

### Load packages

```{r eval=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(bigrquery)
library(ggplot2)
library(tidyr)
library(lubridate) # handle date information
library(readr) # read txt file with credentials
library(fable)
library(tsibble)
library(feasts)
library(ggpmisc) # plot R^2 in scatter plot
```

### Retrieve Google Cloud credentials
Retrieve Google Cloud project ID, which I store in an external file (not uploaded to github).
```{r set up credentials, eval = TRUE}
# Set Google Cloud project ID here
read_creds <- function(){
    project_name <- read_file("./GC_credentials.txt")
    return(project_name)
}
```


### Retrieve data with date information
The statement needs to be written in Standard SQL. 
More info in the [BigQuery documentation](cloud.google.com/bigquery/docs/reference/standard-sql/) 
 
```{r setup crime data request, eval = TRUE, echo = FALSE, message=FALSE, warning=FALSE}
build_crime_SQL_query <- function(){
  sql <- "SELECT iucr, primary_type, description, date FROM `bigquery-public-data.chicago_crime.crime`"
  return(sql)
}
```

TODO: Cache locally and only fetch additional (recent) data.

# Execute the query and store the result
```{r execute SQL call for crime data}
get_crime_data <- function(){
  billing <- read_creds()
  sql <- build_crime_SQL_query()
  tb <- bq_project_query(billing, sql)
  counts_day <- bq_table_download(tb, max_results = Inf)
  return(counts_day)
}
```


Each row in the data stands for one offense of a particular type, at a particular time. We add a column of "counts", that for now is always one. This fascilitates summing up rows (e.g., by day) later on.
```{r add colum of ones --> one row is one offense count}
crime_data_processing <- function(counts_day){
  counts_day$count <- rep(1, nrow(counts_day))
  # format as date (and drop the time part)
  counts_day$date <- as.Date(ymd_hms(counts_day$date))
  # sort by ascending date
  counts_day <- counts_day %>% arrange(date)
  return(counts_day)
}
```

```{r aggregate counts per day}
aggregate_crime_counts_per_day <- function(counts_day){
  # group by day (across offense types)
  counts_day <- tibble(counts_day)

  daily_counts <- counts_day %>%
  group_by(date) %>%
  summarize(counts = sum(count))
  return(daily_counts)
}
```

```{r aggregate counts per offense and day}
aggregate_crime_counts_per_offence_and_day <- function(counts_day){
  daily_counts <- counts_day %>%
  # TODO --> more distinct descriptions than iucr, not sure what's kept in this step
  group_by(date, iucr, primary_type, description) %>%
  summarize(counts = sum(count))
  return(daily_counts)
}
```

The sum of all days should be equal to the initial number of rows of data (i.e., we didn't loose any information in the above data transformation)
```{r sanitiy check, purl=FALSE}
sum(daily_counts$counts) == nrow(counts_day)
```

### tsibble: time-sensitive tibble
1. Index is a variable with inherent ordering from past to present.
2. Key is a set of variables that define observational units over time.
3. Each observation should be uniquely identified by index and key.
4. Each observational unit should be measured at a common interval, if regularly spaced.
```{r time-aware tibble, purl=FALSE}
# turn into time-sensitive tibble
# works like a normal tibble but tracks time (set by index)
daily_counts <- as_tsibble(daily_counts, index=date) %>% arrange(date)
```


```{r main function to get total crime data per day}
get_crime_per_day <- function(){
  counts_day <- get_crime_data()
  print("Started data processing")
  counts_day <- crime_data_processing(counts_day)
  print("Aggregating now by day")
  daily_counts_per_day <- aggregate_crime_counts_per_day(counts_day)
  daily_counts_counts_per_day <- as_tsibble(daily_counts_per_day, index=date) %>% arrange(date)
  return(daily_counts_per_day)
}
```

```{r main function to get crime data per offense type and day}
get_crime_counts_per_type_and_day <- function(){
  counts_day <- get_crime_data()
  print("Started data processing")
  counts_day <- crime_data_processing(counts_day)
  print("Aggregating now by day and offense type")
  daily_counts_per_type_day <- aggregate_crime_counts_per_offence_and_day(counts_day)
  daily_counts_per_type_day  <- as_tsibble(daily_counts_per_type_day , index=date, key=c(iucr, primary_type, description))
  return(daily_counts_per_type_day)
}
```

## Retrieve the weather data

Before pulling data into R, I used the BigQuery online UI to explore available stations. Most stations have a restricted runtime. That means no single station in Chicago ran the entire time period for which we have crime information (01-01-2001 to today).

First, I explored the 'stations' table and realized that multiple stations report weather data for the city Chicago. We can create a geographical quadrant (using lon/lat to select all stations that report data for our crime area.)

SELECT usaf
FROM bigquery-public-data.noaa_gsod.stations
WHERE state='IL' AND (`lat` BETWEEN 41.6 AND 42) AND (`lon` BETWEEN -88 AND -87)
  AND `end` > '20010000'
ORDER BY `end` ASC;

Eventually, I identified the station named "Chicago" (usaf = '997338') as the station with the longest period of continous weather data (20080101 to last full week from today). The station "CHICAGO/MEIGS" (usaf = '725346') reports data from 19730101 to 20080618 and is, hence, useful to fill in the data from 2001 to 2007.


## Retrieve weather data with date information
```{r setup weather request}
build_weather_sql <- function(){
sql <- "
#standardsql
SELECT
  -- Create a timestamp from the date components.
  timestamp(concat(year,'-',mo,'-',da)) as date,
  -- Replace numerical null values with actual nulls
  ROUND(AVG(IF (temp=9999.9, null, temp)),2) AS temperature,
  ROUND(AVG(IF (visib=999.9, null, visib)),2) AS visibility,
  ROUND(AVG(IF (wdsp='999.9', null, CAST(wdsp AS Float64))),2) AS wind_speed,
  ROUND(AVG(IF (gust=999.9, null, gust)),2) AS wind_gust,
  ROUND(AVG(IF (prcp=99.99, null, prcp)),3) AS precipitation,
  AVG(IF (sndp=999.9, null, sndp)) AS snow_depth
FROM
  `bigquery-public-data.noaa_gsod.gsod*`
WHERE
   `stn` IN 
  (SELECT `usaf`
    FROM bigquery-public-data.noaa_gsod.stations
    WHERE state='IL' AND (`lat` BETWEEN 41.6 AND 42) AND (`lon` BETWEEN -88 AND -87)
      AND `end` > '20010000'
    ORDER BY `end` ASC) AND CAST(YEAR AS INT64) > 2000
 GROUP BY date;
"
}
```


TODO: Cache locally and only fetch additional (recent) data.

```{r execute weather SQL call}
# Execute the query and store the result
fetch_weather_data <- function(){
  sql <- build_weather_sql()
  billing <- read_creds()
  tb <- bq_project_query(billing, sql)
  weather <- bq_table_download(tb, max_results = Inf)
  weather <- arrange(weather, weather$date)
  return(weather)
}
```


# How many days of data do we expect?
```{r get nb of days, purl=FALSE}
max(weather$date) - min(weather$date)
# as.Date("2001-01-01", format="%Y-%m-%d") - as.Date("2020-05-13", format="%Y-%m-%d")
```

Using data from all stations across Chicago, makes it more likely that we have no gaps in the data. The SQL query above averages across stations per day.


Turn data frame into a tsibble.

```{r transform into tsibble}
weather_processing <- function(weather){
  weather$date <- as.Date(weather$date)
  daily_weather <- as_tsibble(weather, index = date, regular = TRUE, interval = day)
  return(daily_weather)
}
```


```{r main function to get weather data}
get_weather_data <- function(){
  weather <- fetch_weather_data()
  daily_weather <- weather_processing(weather)
  return(daily_weather)
}
```


```{r produce an R script form this file that can be sourced elsewhere, purl=FALSE}
# remember to set option purl=FALSE in those chunks above, 
# that you don't want included in R script
knitr::purl("crime_weather_covid_ReadData.Rmd")
```
